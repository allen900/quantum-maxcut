{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Max-Cut with QAOA\n",
    "\n",
    "## Brief Problem Description\n",
    "\n",
    "The problem of interest is the weighted max cut problem. Given a set of vertices and weighted edges connecting some of the vertices, we are interested in separating the vertices into two sets such that the sum of the weights of the edges between the sets is maximized. \n",
    "\n",
    "## QAOA Description and Value of P\n",
    "\n",
    "QAOA works in two stages: a classical stage and a quantum computing stage (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import Qconfig\n",
    "from tqdm import tqdm\n",
    "from random import randint, choice, uniform\n",
    "from math import ceil\n",
    "from statistics import stdev, mean\n",
    "from skopt import gbrt_minimize, dummy_minimize, forest_minimize, gp_minimize\n",
    "from qiskit import register, available_backends, QuantumCircuit, QuantumRegister, \\\n",
    "        ClassicalRegister, execute\n",
    "\n",
    "register(Qconfig.APItoken, Qconfig.config[\"url\"])\n",
    "pbar = None\n",
    "DEBUG = False\n",
    "\n",
    "def debug(string):\n",
    "    if DEBUG:\n",
    "        sys.stdout.write(string)\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Stage:\n",
    "\n",
    "In the classical stage, two parameters gamma and beta are randomly chosen and fed into the quantum computer. The quantum computer returns an expectation value to the classical computer based on those two parameters. Based on what expectation values it has seen and what it has now, the classical computer alters the parameters to optimize the cost function in aim for a minimum expectation value. This repeats until the classical computer reaches a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: remove this, we're not using this code to optimize anymore\n",
    "def run_optimizer(num_nodes, filename=\"results.csv\"):\n",
    "    debug(\"-- Building Graph--\\n\")\n",
    "    g = Graph(num_nodes)\n",
    "    debug(str(g) + \"\\n\")\n",
    "\n",
    "    best, best_val = g.optimal_score()\n",
    "    debug(\"Optimal Solution: %s, %s\\n\" % (best, best_val[0]))\n",
    "\n",
    "    # Initialize and run the algorithm.\n",
    "    gamma_start = uniform(0, 2*np.pi)\n",
    "    beta_start = uniform(0, np.pi)\n",
    "\n",
    "    # minimize wants lower values, so negate expectation.\n",
    "    neg_get_expectatation = lambda x, y: -1 * get_expectation(x, y)\n",
    "\n",
    "    debug(\"\\n-- Starting optimization --\\n\")\n",
    "    try:\n",
    "        res = minimize(neg_get_expectation, [gamma_start, beta_start], args=(g),\n",
    "                options=dict(maxiter=2,disp=True), bounds=[(0, 2*np.pi), (0,np.pi)])\n",
    "    except KeyboardInterrupt:\n",
    "        debug(\"\\nWriting to %s\\n\" % (filename))\n",
    "        g.save_results(filename)\n",
    "    finally:\n",
    "        exit()\n",
    "\n",
    "    debug(\"-- Finished optimization  --\\n\")\n",
    "    debug(\"Gamma: %s, Beta: %s\\n\" % (res.x[0], res.x[1]))\n",
    "    debug(\"Final cost: %s\\n\" % (res.maxcv))\n",
    "\n",
    "    best, best_val = g.optimal_score()\n",
    "    debug(\"Optimal Solution: %s, %s\\n\" % (best, best_val[0]))\n",
    "    debug(\"Best Found Solution: %s, %s\\n\" % (g.currentScore, g.currentBest))\n",
    "\n",
    "    debug(\"\\nWriting to %s\\n\" % (filename))\n",
    "    g.save_results(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantum Stage:\n",
    "\n",
    "In this stage, p + 1 qubits are made based on the parameters gamma and beta along with the cost and driver hamiltonians W and V described below. The qubits are constructed like so: WpVp....W1V1 |Φ>. Where |Φ> is initially p input qubits in the + state and 1 output qubit in the 0 state. To measure the expectation with respect to the cost operator, all the input qubits should be measured and then the probabilities of each state recorded. The weighted average or expectation can then be computed by summing up state probabilities multiplied with state costs like so:\n",
    "```\n",
    "C = P(state ‘000..0’) * cost(‘000..0’) + … + P(state ‘111..1’) * cost(‘111..1’)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Problem Encoding:\n",
    "\n",
    "Given a graph with V vertices and E edges, we could encode a candidate solution as a V-length bitstring, where each bit corresponds to which cut the corresponding node in the graph belongs to. To evaluate cost, we simply iterate through the edges in the graph and, for nodes belonging to different cuts in the given bitstring, we sum the weights of their edges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph():\n",
    "    def __init__(self, N, randomize=True):\n",
    "        ''' Initialize a random graph with N vertices. '''\n",
    "        self.N = N\n",
    "        self.E = 0\n",
    "        self.adj = {n:dict() for n in range(N)}\n",
    "\n",
    "        # For storing information about each run.\n",
    "        self.currentScore = float('-inf')\n",
    "        self.currentBest = \"\"\n",
    "        self.runs = []\n",
    "\n",
    "        # Randomly generate edges\n",
    "        if randomize:\n",
    "            self.randomize()\n",
    "\n",
    "    def randomize(self):\n",
    "        ''' Randomly generate edges for this graph. '''\n",
    "\n",
    "        # Generate list of tuples for all possible directed edges.\n",
    "        all_possible_edges = set([(x,y) for x in range(self.N) for y in range(self.N) if x != y])\n",
    "\n",
    "        # Sanity check, ensuring we generated the correct number of edges.\n",
    "        e_gen = len(all_possible_edges) / 2\n",
    "        e_shd = self.N * (self.N-1) / 2\n",
    "        assert e_gen == e_shd , \"%d != %d\" % (e_gen, e_shd)\n",
    "\n",
    "        # Choose a random number of edges for this graph to have. \n",
    "        # Note, we stop at len/2 because we generated directed edges,\n",
    "        # so each edge counts twice.\n",
    "        num_edges = randint(1, len(all_possible_edges)/2)\n",
    "        for i in range(num_edges):\n",
    "            # Choose an edge, remove it and its directed complement from the list.\n",
    "            e = choice(list(all_possible_edges))\n",
    "            all_possible_edges.remove(e)\n",
    "            all_possible_edges.remove(e[::-1])\n",
    "\n",
    "            # Unpack tuple into vertex ints.\n",
    "            u, v = int(e[0]), int(e[1])\n",
    "\n",
    "            # Choose a random weight for each edge.\n",
    "            weight = randint(1, 100)\n",
    "\n",
    "            #weight = 1\n",
    "            self.add_edge(u, v, weight)\n",
    "\n",
    "\n",
    "    def add_edge(self, u, v, weight):\n",
    "        ''' Add an edge to the graph. '''\n",
    "        self.E += 1\n",
    "        self.adj[u][v] = weight\n",
    "\n",
    "    def get_edges(self):\n",
    "        ''' Get a list of all edges. '''\n",
    "        edges = []\n",
    "        for u in self.adj:\n",
    "            for v in self.adj[u]:\n",
    "                edges.append((u, v, self.adj[u][v]))\n",
    "        return edges\n",
    "\n",
    "    def get_score(self,bitstring):\n",
    "        ''' Score a candidate solution. '''\n",
    "        assert len(bitstring) == self.N\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        # For every edge u,v in the graph, add the weight\n",
    "        # of the edge if u,v belong to different cuts\n",
    "        # given this canddiate solution.\n",
    "\n",
    "        for u in self.adj:\n",
    "            for v in self.adj[u]:\n",
    "                if bitstring[u] != bitstring[v]:\n",
    "                    score += self.adj[u][v]\n",
    "        return score\n",
    "\n",
    "    def optimal_score(self):\n",
    "        '''\n",
    "        Returns (score, solutions) holding the best possible solution to the\n",
    "        MaxCut problem with this graph.\n",
    "        '''\n",
    "\n",
    "        best = 0\n",
    "        best_val = []\n",
    "\n",
    "        # Iterate over all possible candidate bitstrings\n",
    "        # Note: the bitstrings from 0 - N/2 are symmetrically\n",
    "        # equivalent to those above\n",
    "        for i in range(ceil((2 ** self.N)/2)):\n",
    "            # Convert number to 0-padded bitstring.\n",
    "            bitstring = bin(i)[2:]\n",
    "            bitstring = (self.N - len(bitstring)) * \"0\" + bitstring\n",
    "\n",
    "            sc = self.get_score(bitstring)\n",
    "            if sc > best:\n",
    "                best = sc\n",
    "                best_val = [bitstring]\n",
    "            elif sc == best:\n",
    "                best_val.append(bitstring)\n",
    "        return best, best_val\n",
    "\n",
    "    def edges_cut(self, bitstring):\n",
    "        ''' Given a candidate solution, return the number of edges that this solution cuts. '''\n",
    "        num = 0\n",
    "        for u in self.adj:\n",
    "            for v in self.adj[u]:\n",
    "                if bitstring[u] != bitstring[v]:\n",
    "                    num += 1\n",
    "        return num\n",
    "\n",
    "    def update_score(self, bitstring):\n",
    "        ''' Scores the given bitstring and keeps track of best. '''\n",
    "        score = self.get_score(bitstring)\n",
    "        if score > self.currentScore:\n",
    "            self.currentScore = score\n",
    "            self.currentBest = bitstring\n",
    "        return score\n",
    "    \n",
    "    def clear_runs(self):\n",
    "        ''' Clear data from past runs. '''\n",
    "        self.currentScore = float('-inf')\n",
    "        self.currentBest = \"\"\n",
    "        self.runs = []\n",
    "        \n",
    "    def add_run(self, gamma, beta, expected_value):\n",
    "        ''' Save the data from each run iteration. '''\n",
    "        self.runs.append([gamma, beta, expected_value])\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"Graph with %d vertices %d edges.\\nAdjacency List: %s\" % (self.N, self.E, self.adj)\n",
    "\n",
    "#graph encoding sample \n",
    "g = Graph(5)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost and Driver Hamiltonians C and B:\n",
    "\n",
    "The cost hamiltonian V can be expressed by exp(-i x gamma x C), where C is the cost operator that transforms a qubit state |Φ> to C(x1,x1,..xp) * |Φ>. In the case of weighted max cut, we can express the cost operator as a sum of local cost operators which each corresponding to an edge in the graph. For each of the qubits corresponding to the vertices of that edge, we apply a phase of e^(i*w*gamma), where w is the weight of the edge between those two qubits.\n",
    "\n",
    "The driver hamiltonian W can be expressed by exp(-i x beta x B), where B is the an operator that flips all the input qubits (X gate on all p input qubits). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_expectation(x, g, NUM_SHOTS=1024):\n",
    "    # Look for progress bar as a global variable.\n",
    "    global pbar\n",
    "    \n",
    "    gamma, beta = x\n",
    "\n",
    "    debug(\"Cost of Gamma: %s, beta: %s... \" % (gamma, beta))\n",
    "\n",
    "    # Construct quantum circuit.\n",
    "    q = QuantumRegister(g.N)\n",
    "    c = ClassicalRegister(g.N)\n",
    "    qc = QuantumCircuit(q, c)\n",
    "\n",
    "    # Apply hadamard to all inputs.\n",
    "    for i in range(g.N):\n",
    "        qc.h(q[i])\n",
    "\n",
    "    # Apply V for all edges.\n",
    "    for edge in g.get_edges():\n",
    "        u, v, w = edge\n",
    "\n",
    "        # Apply CNots.\n",
    "        qc.cx(q[u], q[v])\n",
    "\n",
    "        qc.u1(gamma*w, q[v])\n",
    "\n",
    "        # Apply CNots.\n",
    "        qc.cx(q[u], q[v])\n",
    "\n",
    "    # Apply W to all vertices.\n",
    "    for i in range(g.N):\n",
    "        qc.h(q[i])\n",
    "        qc.u1(-2*beta, q[i])\n",
    "        qc.h(q[i])\n",
    "\n",
    "\n",
    "    # Measure the qubits (avoiding ancilla).\n",
    "    for i in range(g.N):\n",
    "        qc.measure(q[i], c[i])\n",
    "\n",
    "    # Run the simluator.\n",
    "    job = execute(qc, backend='ibmq_qasm_simulator', shots=NUM_SHOTS)\n",
    "    results = job.result()\n",
    "    result_dict = results.get_counts(qc)\n",
    "\n",
    "    debug(\"done!\\n\")\n",
    "\n",
    "    # Calculate the expected value of the candidate bitstrings.\n",
    "    exp = 0\n",
    "    for bitstring in result_dict:\n",
    "        prob = np.float(result_dict[bitstring]) / NUM_SHOTS\n",
    "        score = g.update_score(bitstring)\n",
    "\n",
    "        # Expected value is the score of each bitstring times\n",
    "        # probability of it occuring.\n",
    "        exp += score * prob\n",
    "\n",
    "    debug(\"\\tExpected Value: %s\\n\" % (exp))\n",
    "    debug(\"\\tBest Found Solution: %s, %s\\n\" % (g.currentScore, g.currentBest))\n",
    "\n",
    "    g.add_run(gamma, beta, exp)\n",
    "\n",
    "    # Try updating progress bar if defined.\n",
    "    try:\n",
    "        pbar.update(1)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return exp # bc we want to minimize\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics (we performed some tests and plotted graphs to verify our choices)\n",
    "\n",
    "## Expectation vs Gamma and Beta:\n",
    "\n",
    "We want to make sure there's actually a gradient, so we plotted how expectation changed with changing gamma and beta.\n",
    "Here's the code for it plus the actual graphs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_constant(vary=\"gamma\"):\n",
    "    ''' Plots expected value vs. gamma/beta, holding the rest of the variables constant.'''\n",
    "\n",
    "\n",
    "    \n",
    "    # Choose some random starting beta/gamma and graph.\n",
    "    lim = np.pi if vary == \"gamma\" else 2*np.pi\n",
    "    constant_var = uniform(0, lim)\n",
    "    g = Graph(5)\n",
    "\n",
    "    # RUNS # of runs at each gamma for error bars.\n",
    "    RUNS = 3\n",
    "\n",
    "    # Keep track of gammas, expected values, for plotting.\n",
    "    pts, exp, std = [], [], []\n",
    "\n",
    "    # The maximum possible expected value is the maximum possible weighted cut.\n",
    "    opt = g.optimal_score()[0]\n",
    "    debug(\"Optimal score: %s\\n\" % (opt))\n",
    "    \n",
    "    # Number of data points to collect.\n",
    "    NUM_RUNS = 3\n",
    "    MIN = 0\n",
    "    MAX = 2*np.pi if vary == \"gamma\" else np.pi\n",
    "    \n",
    "    # For progress bar.\n",
    "    global pbar\n",
    "    pbar = tqdm(total=NUM_RUNS*RUNS)\n",
    "    \n",
    "    points = np.linspace(MIN, MAX, NUM_RUNS)\n",
    "    for point in points:\n",
    "        pts.append(point)\n",
    "\n",
    "        # Calculate expected values.\n",
    "        vals = []\n",
    "        for i in range(RUNS):\n",
    "        \n",
    "            # Params are passed in as gamma, beta, so order matters.\n",
    "            params = [point, constant_var] if vary == \"gamma\" else [constant_var, point]\n",
    "            vals.append(get_expectation(params, g))\n",
    "\n",
    "        # Calculate mean, standard deviation.\n",
    "        exp.append(mean(vals))\n",
    "        std.append(stdev(vals))\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.errorbar(x=pts, y=exp, yerr=std, fmt='o-', markersize=10)\n",
    "    ax.legend(loc=2)\n",
    "\n",
    "    # Names for plotting.\n",
    "    vary_name = \"Gamma\" if vary == \"gamma\" else \"Beta\"\n",
    "    const_name = \"Beta\" if vary_name == \"Gamma\" else \"Gamma\"\n",
    "    \n",
    "    ax.set_title(\"Effect of Varying %s with %s = %s\" % (vary_name, const_name, constant_var))\n",
    "    ax.set_xlabel(\"%s\" % (vary_name)) \n",
    "    ax.set_ylabel(\"Expected Value\")\n",
    "\n",
    "\n",
    "    plt.savefig(\"img/gamma_test.png\")\n",
    "hold_constant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gamma vs. exp](img/gamma_change.png)\n",
    "![beta vs.exp](img/beta_change.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Classical Optimizers:\n",
    "\n",
    "Still not convinced we can actually optimize, we tested a bunch of different things:\n",
    "    * graph size 15\n",
    "    * n calls 8\n",
    "    * 2 restarts\n",
    "Below is the code to do the stuff followed by the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot different types of optimizers.\n",
    "def compare_optimizers(num_instances=4, graph_size=15, n_calls=8, n_random_starts=2):\n",
    "    global pbar\n",
    "    pbar = None\n",
    "    \n",
    "    # For progress bar.\n",
    "    pbar = tqdm(total=num_instances*n_calls*4)\n",
    "    \n",
    "    instances = [Graph(graph_size) for _ in range(num_instances)]\n",
    "    \n",
    "    # Percent of optimal score acheived by each algorithm.\n",
    "    dummy = []\n",
    "    decision_trees = []\n",
    "    gradient_boosted_trees = []\n",
    "    baynesian = []\n",
    "    \n",
    "    # For each instance, run each algorithm.\n",
    "    for inst in instances:\n",
    "        # Scikit functions only take in parameters and want to minimize values.\n",
    "        # Create a wrapper function to format get_expectation.\n",
    "        sk_get_exp = lambda x: -1*get_expectation(x, inst)\n",
    "\n",
    "        \n",
    "        opt = inst.optimal_score()[0]\n",
    "        \n",
    "        # Dummy.\n",
    "        inst.clear_runs()\n",
    "        dummy_minimize(func=sk_get_exp,\n",
    "                      dimensions=[(0,2*np.pi),(0,np.pi)],\n",
    "                      n_calls=n_calls)\n",
    "        dummy.append(float(inst.currentScore) / opt)\n",
    "\n",
    "        # Decision Trees.\n",
    "        inst.clear_runs()\n",
    "        forest_minimize(func=sk_get_exp,\n",
    "                      dimensions=[(0,2*np.pi),(0,np.pi)],\n",
    "                      n_calls=n_calls,\n",
    "                      n_random_starts=n_random_starts)\n",
    "        decision_trees.append(float(inst.currentScore) / opt)\n",
    "        \n",
    "        # Gradient Boosted Decision Trees.\n",
    "        inst.clear_runs()\n",
    "        gbrt_minimize(func=sk_get_exp,\n",
    "                      dimensions=[(0,2*np.pi),(0,np.pi)],\n",
    "                      n_calls=n_calls,\n",
    "                      n_random_starts=n_random_starts)\n",
    "        gradient_boosted_trees.append(float(inst.currentScore) / opt)\n",
    "        \n",
    "        # Baynesian.\n",
    "        inst.clear_runs()\n",
    "        gp_minimize(func=sk_get_exp,\n",
    "                      dimensions=[(0,2*np.pi),(0,np.pi)],\n",
    "                      n_calls=n_calls,\n",
    "                      n_random_starts=n_random_starts)\n",
    "        baynesian.append(float(inst.currentScore) / opt)\n",
    "\n",
    "    # Compare mean/stdev of % opt. achieved for each algorithm.\n",
    "    print(\"-- % Optimal Achieved, Mean and Std. Dev --\")\n",
    "    print(\"Random Sampling:\\nMean: %s\\nStd. Dev: %s\" % (mean(dummy), stdev(dummy)))\n",
    "    print(\"Decision Trees:\\nMean: %s\\nStd. Dev: %s\" % (mean(decision_trees), stdev(decision_trees)))\n",
    "    print(\"Gradient Boosted Decision Trees:\\nMean: %s\\nStd. Dev: %s\" % (mean(gradient_boosted_trees), stdev(gradient_boosted_trees)))\n",
    "    print(\"Baynesian Optimization:\\nMean: %s\\nStd. Dev: %s\" % (mean(baynesian), stdev(baynesian)))\n",
    "    \n",
    "#compare_optimizers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "-- % Optimal Achieved, Mean and Std. Dev --\n",
    "Random Sampling:\n",
    "Mean: 0.9918135656630878\n",
    "Std. Dev: 0.008893516482779347\n",
    "Decision Trees:\n",
    "Mean: 0.9911853052336336\n",
    "Std. Dev: 0.013767350119746134\n",
    "Gradient Boosted Decision Trees:\n",
    "Mean: 0.9874128354367063\n",
    "Std. Dev: 0.01234703421622558\n",
    "Baynesian Optimization:\n",
    "Mean: 0.9976966107272129\n",
    "Std. Dev: 0.00398958725007638\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# woah\n",
    "they're like really like close togetherl ike wooooah so should we even bother optimizng \n",
    "baynesian optimization seems to do the best, but the results are so close, that it doesn't really seem to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Discussion\n",
    "\n",
    "### Minimum, Maximum and Mean Costs across Problem Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_cost(num_instances=20, num_vert=10, num_runs=5):\n",
    "    '''\n",
    "    For several random problem instances, plot the cost of the output state.\n",
    "    Plot average, maximum and minimum cost.\n",
    "    '''\n",
    "\n",
    "    # Prepare several random instances of the problem.\n",
    "    instances = [Graph(num_vert) for _ in range(num_instances)]\n",
    "\n",
    "    # Choose starting values for gamma and beta.\n",
    "\n",
    "    # For holding iteration number and expected values.\n",
    "    its, exps, opts, best_founds = [], [], [], []\n",
    "\n",
    "    it = 1\n",
    "    # Calculate expected values.\n",
    "    for graph in tqdm(instances):\n",
    "        vals = []\n",
    "        for _ in range(num_runs):\n",
    "            # Use random gamma, beta for each run.\n",
    "            gamma = uniform(0, 2*np.pi)\n",
    "            beta = uniform(0, np.pi)\n",
    "\n",
    "            vals.append(get_expectation([gamma, beta], graph))\n",
    "\n",
    "        # Save results.\n",
    "        its.append(it)\n",
    "        exps.append(vals)\n",
    "        curr_opt = graph.optimal_score()[0]\n",
    "        opts.append(curr_opt)\n",
    "        best_founds.append(float(graph.currentScore) / curr_opt)\n",
    "        it += 1\n",
    "\n",
    "\n",
    "    plt.title(\"Costs of Random Instances\")\n",
    "    plt.xlabel(\"Instance Iteration Number\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "\n",
    "    '''abandoned avenue\n",
    "    # Sort by optimal value just so it's pleasant to look at.\n",
    "    #exps = [x for _,x in sorted(zip(opt,exps), key=lambda pair: pair[0])]\n",
    "    #opt = sorted(opt)\n",
    "    '''\n",
    "    \n",
    "    averages = [mean(ex)/opt for ex, opt in zip(exps, opts)]\n",
    "    lows = [min(ex)/opt for ex, opt in zip(exps, opts)]\n",
    "    highs = [max(ex)/opt for ex, opt in zip(exps, opts)]\n",
    "\n",
    "    plt.plot(its, averages, color='blue', label='Average Cost %')\n",
    "    plt.plot(its, lows, color='green', label='Minimum Cost %')\n",
    "    plt.plot(its, highs, color='orange', label='Maximum Cost %')\n",
    "    plt.plot(its, best_founds, color='red', label='Best Found Cost %')\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cost Variation vs. Instance Iteration](img/gamma_change.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
